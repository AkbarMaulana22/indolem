{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from conllu import parse\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "random.seed(2020)\n",
    "train = parse(open('id_gsd-ud-train.conllu', 'r').read())\n",
    "dev = parse(open('id_gsd-ud-dev.conllu', 'r').read())\n",
    "test = parse(open('id_gsd-ud-test.conllu', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4477\n",
      "dev: 559\n",
      "test: 557\n"
     ]
    }
   ],
   "source": [
    "print('train:',len(train))\n",
    "print('dev:',len(dev))\n",
    "print('test:',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121923\n"
     ]
    }
   ],
   "source": [
    "# Format change: Word_BERT_sentenceID_wordID\n",
    "word2index = {}\n",
    "idx=0\n",
    "words = defaultdict(int)\n",
    "for sentence in train:\n",
    "    suffix = '_BERT_'+sentence.metadata['sent_id']\n",
    "    for word in sentence:\n",
    "        cur_suffix = suffix + '_' + str(word['id'])\n",
    "        new_word = word['form'] + cur_suffix  \n",
    "        word['form'] = new_word\n",
    "        word2index[new_word]=idx\n",
    "        idx+=1\n",
    "for sentence in test:\n",
    "    suffix = '_BERT_'+sentence.metadata['sent_id']\n",
    "    for word in sentence:\n",
    "        cur_suffix = suffix + '_' + str(word['id'])\n",
    "        new_word = word['form'] + cur_suffix  \n",
    "        word['form'] = new_word\n",
    "        word2index[new_word]=idx\n",
    "        idx+=1\n",
    "for sentence in dev:\n",
    "    suffix = '_BERT_'+sentence.metadata['sent_id']\n",
    "    for word in sentence:\n",
    "        cur_suffix = suffix + '_' + str(word['id'])\n",
    "        new_word = word['form'] + cur_suffix  \n",
    "        word['form'] = new_word\n",
    "        word2index[new_word]=idx\n",
    "        idx+=1\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "os.makedirs('data/')\n",
    "with open('data/word2index.json', 'w') as file:\n",
    "    json.dump(word2index, file)\n",
    "with open('data/word2index.json', 'r') as file:\n",
    "    new_d = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bert(sentence, model, tokz, save_to, word2index):\n",
    "    os.makedirs(save_to)\n",
    "    indexes = []\n",
    "    subtokens = ['[CLS]']\n",
    "    for word in sentence:\n",
    "        indexes.append(len(subtokens))\n",
    "        real_word = word['form'].split('_BERT_')[0]\n",
    "        subtoken = tokz.tokenize(real_word)\n",
    "        subtokens += subtoken\n",
    "    subtokens += ['[SEP]']\n",
    "    \n",
    "    subtoken_ids = tokz.convert_tokens_to_ids(subtokens)\n",
    "    segment_ids = [0] * len(subtokens)\n",
    "    \n",
    "    subtoken_ids_t = torch.tensor(subtoken_ids).unsqueeze(0)\n",
    "    segment_ids_t = torch.tensor(segment_ids).unsqueeze(0)\n",
    "    \n",
    "    output, _ = model(input_ids=subtoken_ids_t, token_type_ids=segment_ids_t)\n",
    "    output = output.view(len(subtoken_ids), model.config.hidden_size)\n",
    "    indexes = torch.tensor(indexes)\n",
    "    selected_output = torch.index_select(output, 0, indexes)\n",
    "    \n",
    "    assert len(indexes) == len(sentence) == selected_output.shape[0]\n",
    "    for idx in range(len(sentence)):\n",
    "        word = sentence[idx]['form']\n",
    "        array = selected_output[idx].data.numpy()\n",
    "        np.save(save_to+str(word2index[word]), array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract mBERT embedding\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-multilingual-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
    "save_to='data/mbert/'\n",
    "\n",
    "for sentence in train:\n",
    "    extract_bert(sentence, bert, tokenizer, save_to, word2index)\n",
    "for sentence in test:\n",
    "    extract_bert(sentence, bert, tokenizer, save_to, word2index)\n",
    "for sentence in dev:\n",
    "    extract_bert(sentence, bert, tokenizer, save_to, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated\n"
     ]
    }
   ],
   "source": [
    "#Extract IndoBERT embedding\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('indolem/indobert-base-uncased', do_lower_case=True)\n",
    "bert = BertModel.from_pretrained('indolem/indobert-base-uncased')\n",
    "save_to='data/indobert/'\n",
    "\n",
    "for sentence in train:\n",
    "    extract_bert(sentence, bert, tokenizer, save_to, word2index)\n",
    "for sentence in test:\n",
    "    extract_bert(sentence, bert, tokenizer, save_to, word2index)\n",
    "for sentence in dev:\n",
    "    extract_bert(sentence, bert, tokenizer, save_to, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract MalayBERT embedding, Only cased-malay-bert is available\n",
    "from transformers import AlbertTokenizer, BertModel\n",
    "\n",
    "tokenizer = AlbertTokenizer.from_pretrained('huseinzol05/bert-base-bahasa-cased', \n",
    "                unk_token = '[UNK]', pad_token='[PAD]', do_lower_case=False)\n",
    "bert = BertModel.from_pretrained('huseinzol05/bert-base-bahasa-cased')\n",
    "save_to='data/malaybert/'\n",
    "\n",
    "for sentence in train:\n",
    "    extract_bert(sentence, bert, tokenizer, save_to, word2index)\n",
    "for sentence in test:\n",
    "    extract_bert(sentence, bert, tokenizer, save_to, word2index)\n",
    "for sentence in dev:\n",
    "    extract_bert(sentence, bert, tokenizer, save_to, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(conlls, fname):\n",
    "    f = open(fname, 'w')\n",
    "    for conll in conlls:\n",
    "        f.write(conll.serialize())\n",
    "    f.close()\n",
    "    \n",
    "write(train, 'data/train.conllu')\n",
    "write(test, 'data/test.conllu')\n",
    "write(dev, 'data/dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
