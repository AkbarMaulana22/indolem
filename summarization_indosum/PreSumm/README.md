# PreSumm

**This code is originally for EMNLP 2019 paper [Text Summarization with Pretrained Encoders](https://arxiv.org/abs/1908.08345)**

We modify the code and adjust the utility for three BERT models. 
