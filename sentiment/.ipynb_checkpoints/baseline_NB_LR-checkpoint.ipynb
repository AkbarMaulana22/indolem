{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bpe import Encoder\n",
    "\n",
    "alpha_vals = np.linspace(0.001,1,100)\n",
    "\n",
    "def tuning_nb(xtrain, ytrain, xdev, ydev):\n",
    "    alpha_params = {}\n",
    "    for alpha in alpha_vals:\n",
    "        model = MultinomialNB(alpha = alpha)\n",
    "        model.fit(xtrain,ytrain)\n",
    "        pred = model.predict(xdev)\n",
    "        alpha_params[alpha] = f1_score(ydev,pred)\n",
    "    maxAlpha = max(alpha_params, key=alpha_params.get) \n",
    "    print('After tuning NB for', len(alpha_vals), 'we get param:', maxAlpha)\n",
    "    return maxAlpha\n",
    "\n",
    "def train_and_test_nb(xtrain, ytrain, xtest, ytest, xdev, ydev):\n",
    "    encoder = Encoder(5000, pct_bpe=0.88)\n",
    "    encoder.fit(xtrain)\n",
    "    xtrain = [' '.join(encoder.tokenize(name)) for name in xtrain]\n",
    "    xtest = [' '.join(encoder.tokenize(name)) for name in xtest]\n",
    "    xdev = [' '.join(encoder.tokenize(name)) for name in xdev]\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2), lowercase=False)\n",
    "    x_train = vectorizer.fit_transform(xtrain)\n",
    "    x_test = vectorizer.transform(xtest)\n",
    "    x_dev = vectorizer.transform(xdev)\n",
    "    \n",
    "    maxAlpha = tuning_nb(x_train, ytrain, x_dev, ydev)\n",
    "    clf = MultinomialNB(maxAlpha)\n",
    "    clf.fit(x_train.toarray(), ytrain)\n",
    "    pred = clf.predict(x_test.toarray())\n",
    "    f1score = f1_score(ytest,pred)\n",
    "    return f1score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_vals = np.linspace(0.001,10,100)\n",
    "\n",
    "def tuning_lr(xtrain, ytrain, xdev, ydev):\n",
    "    c_params = {}\n",
    "    for c in c_vals:\n",
    "        model = LogisticRegression(C = c)\n",
    "        model.fit(xtrain,ytrain)\n",
    "        pred = model.predict(xdev)\n",
    "        c_params[c] = f1_score(ydev,pred)\n",
    "    maxC = max(c_params, key=c_params.get) \n",
    "    print('After tuning LR for', len(alpha_vals), 'we get param:', maxC)\n",
    "    return maxC\n",
    "\n",
    "def train_and_test_lr(xtrain, ytrain, xtest, ytest, xdev, ydev):\n",
    "    encoder = Encoder(1000, pct_bpe=0.88)\n",
    "    encoder.fit(xtrain)\n",
    "    xtrain = [' '.join(encoder.tokenize(name)) for name in xtrain]\n",
    "    xtest = [' '.join(encoder.tokenize(name)) for name in xtest]\n",
    "    xdev = [' '.join(encoder.tokenize(name)) for name in xdev]\n",
    "    \n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 2), lowercase=False)\n",
    "    x_train = vectorizer.fit_transform(xtrain)\n",
    "    x_test = vectorizer.transform(xtest)\n",
    "    x_dev = vectorizer.transform(xdev)\n",
    "    \n",
    "    maxC = tuning_lr(x_train, ytrain, x_dev, ydev)\n",
    "    clf = LogisticRegression(C=maxC)\n",
    "    clf.fit(x_train.toarray(), ytrain)\n",
    "    pred = clf.predict(x_test.toarray())\n",
    "    f1score = f1_score(ytest,pred)\n",
    "    return f1score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive Bayes')\n",
    "f1s = 0.0\n",
    "for idx in range(5):\n",
    "    train = pd.read_csv('train'+str(idx)+'.csv')\n",
    "    test = pd.read_csv('test'+str(idx)+'.csv')\n",
    "    dev = pd.read_csv('dev'+str(idx)+'.csv')\n",
    "    xtrain, ytrain = list(train['name']), list(train['gender'])\n",
    "    xtest, ytest = list(test['name']), list(test['gender'])\n",
    "    xdev, ydev = list(dev['name']), list(dev['gender'])\n",
    "    f1s += train_and_test_nb(xtrain, ytrain, xtest, ytest, xdev, ydev)\n",
    "print(f1s/5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic Regression')\n",
    "f1s = 0.0\n",
    "for idx in range(5):\n",
    "    train = pd.read_csv('train'+str(idx)+'.csv')\n",
    "    test = pd.read_csv('test'+str(idx)+'.csv')\n",
    "    dev = pd.read_csv('dev'+str(idx)+'.csv')\n",
    "    xtrain, ytrain = list(train['name']), list(train['gender'])\n",
    "    xtest, ytest = list(test['name']), list(test['gender'])\n",
    "    xdev, ydev = list(dev['name']), list(dev['gender'])\n",
    "    f1s += train_and_test_lr(xtrain, ytrain, xtest, ytest, xdev, ydev)\n",
    "print(f1s/5.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
