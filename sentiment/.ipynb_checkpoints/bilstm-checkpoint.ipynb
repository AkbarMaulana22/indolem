{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re, tensorflow.keras, os\n",
    "import pandas as pd, keras, io\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, Input,Dropout, Embedding, LSTM, Bidirectional, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, AveragePooling1D, TimeDistributed, GlobalMaxPooling1D, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from bpe import Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['MKL_NUM_THREADS'] = '4'\n",
    "os.environ['GOTO_NUM_THREADS'] = '4'\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORD_LEN=150 \n",
    "MAX_NB_WORDS=50000\n",
    "EMBEDDING_DIM=300\n",
    "NUM_CLASS=2\n",
    "PATIENCE = 20\n",
    "ITERATIONS = 100\n",
    "BATCH_SIZE = 100\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_vectors(fname, word_index):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        if word_index.get(tokens[0],-1) != -1:\n",
    "            data[tokens[0]] = list(map(float, tokens[1:]))\n",
    "    return data\n",
    "\n",
    "\n",
    "def model_with_fasttext(x_train, y_train, x_dev, y_dev, x_test, y_test, tokenizer):\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    print('Total words in dict:', nb_words)\n",
    "    embeddings = load_vectors('../cc.id.300.vec', word_index)\n",
    "    embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_NB_WORDS:\n",
    "            continue\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            embedding_matrix[i] = np.random.normal(-4.2, 4.2, EMBEDDING_DIM)\n",
    "\n",
    "    # MODEL \n",
    "    # your code here\n",
    "    with tf.device('/gpu:0'):\n",
    "        embedding_layer = Embedding(nb_words + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                                input_length=MAX_WORD_LEN, trainable=False)\n",
    "        \n",
    "        tweet = Input(shape=(MAX_WORD_LEN,), dtype='int32')\n",
    "        embedded_sequences = embedding_layer(tweet)\n",
    "\n",
    "        lstm_cell = LSTM(units=200, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "                recurrent_regularizer=keras.regularizers.l2(0.2), return_sequences=False, dropout=0.3, recurrent_dropout=0.3)\n",
    "        doc_vector = Bidirectional(lstm_cell, merge_mode='concat')(embedded_sequences)\n",
    "        \n",
    "        \n",
    "        sign = Dense(NUM_CLASS, activation='softmax')(doc_vector)\n",
    "        sent_model = Model([tweet], [sign])\n",
    "        sent_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "        bestf1=0.0; patience = 0\n",
    "        for i in range(ITERATIONS):\n",
    "            if patience is PATIENCE:\n",
    "                break\n",
    "            sent_model.fit([x_train], [y_train], batch_size=BATCH_SIZE, \n",
    "                       epochs=1, shuffle=True, verbose=False)\n",
    "            prediction=sent_model.predict([x_dev], batch_size=1000)\n",
    "            predicted_label = np.argmax(prediction,axis=1)\n",
    "            f1score = f1_score(y_dev,predicted_label)\n",
    "            if f1score > bestf1:\n",
    "                print('Epoch ' + str(i) +' with dev f1: '+ str(f1score))\n",
    "                bestf1 = f1score\n",
    "                sent_model.save('save.keras')\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "        sent_model = load_model('save.keras')\n",
    "        prediction=sent_model.predict([x_test], batch_size=1000)\n",
    "        predicted_label = np.argmax(prediction,axis=1)\n",
    "    f1score = f1_score(y_test,predicted_label)\n",
    "    print('Test F1: ',f1score)\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    return f1score\n",
    "\n",
    "\n",
    "def model(x_train, y_train, x_dev, y_dev, x_test, y_test, tokenizer):\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "    print('Total words in dict:', nb_words)\n",
    "    embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_matrix[i] = np.random.normal(-4.2, 4.2, EMBEDDING_DIM)\n",
    "\n",
    "    embedding_layer = Embedding(nb_words + 1, EMBEDDING_DIM, weights=[embedding_matrix], \n",
    "                                input_length=MAX_WORD_LEN, trainable=False)\n",
    "    \n",
    "    # MODEL \n",
    "    with tf.device('/gpu:0'):\n",
    "        tweet = Input(shape=(MAX_WORD_LEN,), dtype='int32')\n",
    "        embedded_sequences = embedding_layer(tweet)\n",
    "\n",
    "        lstm_cell = LSTM(units=200, activation='tanh', recurrent_activation='hard_sigmoid', \n",
    "                recurrent_regularizer=keras.regularizers.l2(0.2), return_sequences=False)\n",
    "        doc_vector = Bidirectional(lstm_cell, merge_mode='concat')(embedded_sequences)\n",
    "\n",
    "        sign = Dense(NUM_CLASS, activation='softmax')(doc_vector)\n",
    "        sent_model = Model([tweet], [sign])\n",
    "        sent_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "        bestf1=0.0; patience = 0\n",
    "        for i in range(ITERATIONS):\n",
    "            if patience is PATIENCE:\n",
    "                break\n",
    "            sent_model.fit([x_train], [y_train], batch_size=BATCH_SIZE, \n",
    "                       epochs=1, shuffle=True, verbose=False)\n",
    "            prediction=sent_model.predict([x_dev], batch_size=1000)\n",
    "            predicted_label = np.argmax(prediction,axis=1)\n",
    "            f1score = f1_score(y_dev,predicted_label)\n",
    "            if f1score > bestf1:\n",
    "                print('Epoch ' + str(i) +' with dev f1: '+ str(f1score))\n",
    "                bestf1 = f1score\n",
    "                sent_model.save('save.keras')\n",
    "                patience = 0\n",
    "            else:\n",
    "                patience += 1\n",
    "        sent_model = load_model('save.keras')\n",
    "        prediction=sent_model.predict([x_test], batch_size=1000)\n",
    "        predicted_label = np.argmax(prediction,axis=1)\n",
    "    f1score = f1_score(y_test,predicted_label)\n",
    "    print('Test F1: ',f1score)\n",
    "    print('-----------------------------------------------------------------------------------')\n",
    "    return f1score\n",
    "\n",
    "def train_and_test_fasttext(x_train, y_train, x_dev, y_dev, x_test, y_test):    \n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n",
    "    tokenizer.fit_on_texts(x_train)\n",
    "    \n",
    "    x_train = tokenizer.texts_to_sequences(x_train)\n",
    "    x_test = tokenizer.texts_to_sequences(x_test)\n",
    "    x_dev = tokenizer.texts_to_sequences(x_dev)\n",
    "    \n",
    "    max_len = max([len(t) for t in x_train])\n",
    "    print ('Max Len', max_len)\n",
    "    max_len = MAX_WORD_LEN\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_len, padding='post')\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=max_len, padding='post')\n",
    "    x_dev = sequence.pad_sequences(x_dev, maxlen=max_len, padding='post')\n",
    "    return model_with_fasttext(x_train, to_categorical(y_train), x_dev, y_dev, x_test, y_test, tokenizer)\n",
    "\n",
    "def train_and_test(x_train, y_train, x_dev, y_dev, x_test, y_test):   \n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(x_train)\n",
    "    \n",
    "    x_train = tokenizer.texts_to_sequences(x_train)\n",
    "    x_test = tokenizer.texts_to_sequences(x_test)\n",
    "    x_dev = tokenizer.texts_to_sequences(x_dev)\n",
    "    \n",
    "    max_len = max([len(t) for t in x_train])\n",
    "    print ('Max Len', max_len)\n",
    "    #assert max_len < MAX_WORD_LEN\n",
    "    max_len = MAX_WORD_LEN\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_len, padding='post')\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=max_len, padding='post')\n",
    "    x_dev = sequence.pad_sequences(x_dev, maxlen=max_len, padding='post')\n",
    "    return model(x_train, to_categorical(y_train), x_dev, y_dev, x_test, y_test, tokenizer)\n",
    "\n",
    "def train_and_test_bpe(x_train, y_train, x_dev, y_dev, x_test, y_test):\n",
    "    encoder = Encoder(30000, pct_bpe=0.5)  # params chosen for demonstration purposes\n",
    "    encoder.fit(xtrain)\n",
    "    x_train = [' '.join(encoder.tokenize(name)) for name in x_train]\n",
    "    x_test = [' '.join(encoder.tokenize(name)) for name in x_test]\n",
    "    x_dev = [' '.join(encoder.tokenize(name)) for name in x_dev]\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(x_train)\n",
    "    \n",
    "    x_train = tokenizer.texts_to_sequences(x_train)\n",
    "    x_test = tokenizer.texts_to_sequences(x_test)\n",
    "    x_dev = tokenizer.texts_to_sequences(x_dev)\n",
    "    \n",
    "    max_len = max([len(t) for t in x_train])\n",
    "    print ('Max Len', max_len)\n",
    "    max_len = MAX_WORD_LEN\n",
    "    x_train = sequence.pad_sequences(x_train, maxlen=max_len, padding='post')\n",
    "    x_test = sequence.pad_sequences(x_test, maxlen=max_len, padding='post')\n",
    "    x_dev = sequence.pad_sequences(x_dev, maxlen=max_len, padding='post')\n",
    "    return model(x_train, to_categorical(y_train), x_dev, y_dev, x_test, y_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size 100\n",
      "Max Len 139\n",
      "Total words in dict: 10450\n",
      "Epoch 0 with dev f1: 0.3837209302325581\n",
      "Epoch 2 with dev f1: 0.5463917525773195\n",
      "Epoch 3 with dev f1: 0.6027397260273972\n",
      "Epoch 4 with dev f1: 0.6293103448275863\n",
      "Epoch 8 with dev f1: 0.646288209606987\n",
      "Epoch 10 with dev f1: 0.6576576576576577\n",
      "Test F1:  0.7241379310344828\n",
      "-----------------------------------------------------------------------------------\n",
      "Max Len 133\n",
      "Total words in dict: 10434\n",
      "Epoch 0 with dev f1: 0.23611111111111113\n",
      "Epoch 1 with dev f1: 0.4607329842931937\n",
      "Epoch 2 with dev f1: 0.5170731707317073\n",
      "Epoch 3 with dev f1: 0.5739910313901345\n",
      "Epoch 5 with dev f1: 0.5909090909090909\n",
      "Epoch 7 with dev f1: 0.701492537313433\n",
      "Test F1:  0.7128129602356407\n",
      "-----------------------------------------------------------------------------------\n",
      "Max Len 139\n",
      "Total words in dict: 10682\n",
      "Epoch 0 with dev f1: 0.28187919463087246\n",
      "Epoch 1 with dev f1: 0.3647798742138365\n",
      "Epoch 2 with dev f1: 0.5688888888888889\n",
      "Epoch 5 with dev f1: 0.7063829787234043\n",
      "Epoch 8 with dev f1: 0.7257383966244726\n",
      "Test F1:  0.7119205298013245\n",
      "-----------------------------------------------------------------------------------\n",
      "Max Len 139\n",
      "Total words in dict: 10580\n",
      "Epoch 0 with dev f1: 0.2857142857142857\n",
      "Epoch 1 with dev f1: 0.5380710659898477\n",
      "Epoch 2 with dev f1: 0.5865384615384616\n",
      "Epoch 7 with dev f1: 0.6280193236714976\n",
      "Epoch 8 with dev f1: 0.7118644067796611\n",
      "Test F1:  0.7116968698517299\n",
      "-----------------------------------------------------------------------------------\n",
      "Max Len 139\n",
      "Total words in dict: 10561\n",
      "Epoch 0 with dev f1: 0.3773584905660377\n",
      "Epoch 1 with dev f1: 0.4311377245508982\n",
      "Epoch 2 with dev f1: 0.7114624505928853\n",
      "Epoch 9 with dev f1: 0.7155172413793105\n",
      "Test F1:  0.7360861759425493\n",
      "-----------------------------------------------------------------------------------\n",
      "0.7193308933731454\n"
     ]
    }
   ],
   "source": [
    "print('Batch Size', BATCH_SIZE)\n",
    "f1s = 0.0\n",
    "for idx in range(5):\n",
    "    train = pd.read_csv('train'+str(idx)+'.csv')\n",
    "    dev = pd.read_csv('dev'+str(idx)+'.csv')\n",
    "    test = pd.read_csv('test'+str(idx)+'.csv')\n",
    "    xtrain, ytrain = list(train['sentence']), list(train['sentiment'])\n",
    "    xdev, ydev = list(dev['sentence']), list(dev['sentiment'])\n",
    "    xtest, ytest = list(test['sentence']), list(test['sentiment'])\n",
    "    f1s += train_and_test(xtrain, ytrain, xdev, ydev, xtest, ytest)\n",
    "print(f1s/5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size 100\n",
      "Max Len 139\n",
      "Total words in dict: 10406\n",
      "Epoch 0 with dev f1: 0.4157303370786517\n",
      "Epoch 1 with dev f1: 0.5560538116591928\n",
      "Epoch 2 with dev f1: 0.5714285714285714\n",
      "Epoch 3 with dev f1: 0.5803108808290156\n",
      "Epoch 4 with dev f1: 0.631578947368421\n",
      "Epoch 5 with dev f1: 0.6574074074074073\n",
      "Epoch 16 with dev f1: 0.6575342465753424\n",
      "Epoch 17 with dev f1: 0.6605504587155964\n",
      "Epoch 19 with dev f1: 0.6757990867579907\n",
      "Test F1:  0.7001733102253033\n",
      "-----------------------------------------------------------------------------------\n",
      "Max Len 133\n",
      "Total words in dict: 10391\n",
      "Epoch 0 with dev f1: 0.44198895027624313\n",
      "Epoch 2 with dev f1: 0.6375545851528384\n",
      "Epoch 6 with dev f1: 0.6554621848739496\n",
      "Epoch 7 with dev f1: 0.689655172413793\n",
      "Test F1:  0.728171334431631\n",
      "-----------------------------------------------------------------------------------\n",
      "Max Len 139\n",
      "Total words in dict: 10630\n",
      "Epoch 0 with dev f1: 0.32499999999999996\n",
      "Epoch 1 with dev f1: 0.5676855895196506\n",
      "Epoch 2 with dev f1: 0.5816326530612245\n",
      "Epoch 4 with dev f1: 0.6048780487804879\n",
      "Epoch 5 with dev f1: 0.6350710900473934\n",
      "Epoch 6 with dev f1: 0.6448598130841121\n",
      "Epoch 9 with dev f1: 0.7079646017699116\n",
      "Epoch 10 with dev f1: 0.7364016736401674\n",
      "Epoch 11 with dev f1: 0.7368421052631577\n",
      "Epoch 13 with dev f1: 0.7499999999999999\n",
      "Epoch 21 with dev f1: 0.752136752136752\n",
      "Epoch 29 with dev f1: 0.7704918032786886\n",
      "Test F1:  0.7210884353741497\n",
      "-----------------------------------------------------------------------------------\n",
      "Max Len 139\n",
      "Total words in dict: 10537\n",
      "Epoch 0 with dev f1: 0.2535211267605634\n",
      "Epoch 2 with dev f1: 0.4327485380116958\n",
      "Epoch 3 with dev f1: 0.6036036036036035\n",
      "Epoch 6 with dev f1: 0.6346153846153846\n",
      "Epoch 7 with dev f1: 0.6755555555555557\n",
      "Epoch 8 with dev f1: 0.6757990867579907\n",
      "Epoch 20 with dev f1: 0.6812227074235807\n",
      "Test F1:  0.7170474516695957\n",
      "-----------------------------------------------------------------------------------\n",
      "Max Len 139\n",
      "Total words in dict: 10522\n",
      "Epoch 0 with dev f1: 0.32679738562091504\n",
      "Epoch 1 with dev f1: 0.4623115577889447\n",
      "Epoch 2 with dev f1: 0.5025641025641026\n",
      "Epoch 4 with dev f1: 0.6413502109704642\n",
      "Epoch 6 with dev f1: 0.6781115879828326\n",
      "Epoch 8 with dev f1: 0.7000000000000001\n",
      "Epoch 11 with dev f1: 0.7043478260869565\n",
      "Test F1:  0.7056603773584906\n",
      "-----------------------------------------------------------------------------------\n",
      "0.714428181811834\n"
     ]
    }
   ],
   "source": [
    "print('Batch Size', BATCH_SIZE)\n",
    "f1s = 0.0\n",
    "for idx in range(5):\n",
    "    train = pd.read_csv('train'+str(idx)+'.csv')\n",
    "    dev = pd.read_csv('dev'+str(idx)+'.csv')\n",
    "    test = pd.read_csv('test'+str(idx)+'.csv')\n",
    "    xtrain, ytrain = list(train['sentence']), list(train['sentiment'])\n",
    "    xdev, ydev = list(dev['sentence']), list(dev['sentiment'])\n",
    "    xtest, ytest = list(test['sentence']), list(test['sentiment'])\n",
    "    f1s += train_and_test_bpe(xtrain, ytrain, xdev, ydev, xtest, ytest)\n",
    "print(f1s/5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size 100\n",
      "Max Len 139\n",
      "Total words in dict: 10450\n",
      "Epoch 0 with dev f1: 0.06611570247933884\n",
      "Epoch 1 with dev f1: 0.128\n",
      "Epoch 2 with dev f1: 0.32214765100671144\n",
      "Epoch 3 with dev f1: 0.44025157232704404\n",
      "Epoch 4 with dev f1: 0.5333333333333333\n",
      "Epoch 5 with dev f1: 0.5882352941176471\n",
      "Epoch 7 with dev f1: 0.6339285714285714\n",
      "Epoch 11 with dev f1: 0.6442307692307693\n",
      "Epoch 13 with dev f1: 0.6457399103139014\n",
      "Epoch 18 with dev f1: 0.6869565217391305\n",
      "Epoch 35 with dev f1: 0.6972477064220184\n",
      "Test F1:  0.7027972027972028\n",
      "-----------------------------------------------------------------------------------\n",
      "Max Len 133\n",
      "Total words in dict: 10434\n",
      "Epoch 0 with dev f1: 0.04838709677419355\n",
      "Epoch 1 with dev f1: 0.07874015748031497\n",
      "Epoch 2 with dev f1: 0.24161073825503354\n",
      "Epoch 3 with dev f1: 0.34782608695652173\n",
      "Epoch 4 with dev f1: 0.3757575757575758\n",
      "Epoch 5 with dev f1: 0.55\n",
      "Epoch 6 with dev f1: 0.5656565656565656\n",
      "Epoch 7 with dev f1: 0.5951219512195123\n",
      "Epoch 12 with dev f1: 0.7107438016528925\n",
      "Test F1:  0.6936236391912909\n",
      "-----------------------------------------------------------------------------------\n",
      "Max Len 139\n",
      "Total words in dict: 10682\n",
      "Epoch 0 with dev f1: 0.06299212598425198\n",
      "Epoch 2 with dev f1: 0.31578947368421056\n",
      "Epoch 3 with dev f1: 0.44943820224719105\n",
      "Epoch 5 with dev f1: 0.5104166666666666\n",
      "Epoch 6 with dev f1: 0.5133689839572192\n",
      "Epoch 7 with dev f1: 0.6376811594202899\n",
      "Epoch 12 with dev f1: 0.6844444444444444\n",
      "Epoch 29 with dev f1: 0.6877828054298643\n",
      "Epoch 42 with dev f1: 0.6940639269406393\n"
     ]
    }
   ],
   "source": [
    "print('Batch Size', BATCH_SIZE)\n",
    "f1s = 0.0\n",
    "for idx in range(5):\n",
    "    train = pd.read_csv('train'+str(idx)+'.csv')\n",
    "    dev = pd.read_csv('dev'+str(idx)+'.csv')\n",
    "    test = pd.read_csv('test'+str(idx)+'.csv')\n",
    "    xtrain, ytrain = list(train['sentence']), list(train['sentiment'])\n",
    "    xdev, ydev = list(dev['sentence']), list(dev['sentiment'])\n",
    "    xtest, ytest = list(test['sentence']), list(test['sentiment'])\n",
    "    f1s += train_and_test_fasttext(xtrain, ytrain, xdev, ydev, xtest, ytest)\n",
    "print(f1s/5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
